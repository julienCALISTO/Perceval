# this is a manual workflow that compare benchmarks with the same runner and the same version of python
# choose runner, perceval ref, quandelibc ref, save data and log 
# runs benchmarks at benchmarks/benchmark_*.py


name: Benchmarks

on:
  push:
    branches:
      - main
      - release/*
  release:
    types: [ published ]
  workflow_dispatch:
    inputs:
      os:
        description: 'Choose Runner'
        required: true
        default: 'MiniMac_arm64'
        type: choice
        options:
          - MiniMac_arm64
          - ubuntu-latest
      commit_ref:
        description: Use specific perceval's ref (branch, tag or SHA)
        default: ''
        type: string
        required: false
      quandelibc_ref:
        description: Use specific Quandelibc's ref (branch, tag or SHA)
        default: ''
        type: string
        required: false
      save:
        description: 'commit data'
        default: false
        required: false
        type: boolean

env:
  python_v: ${{ github.event.inputs.python_v || '3.9' }}
  save_log: ${{ github.event.inputs.save_log || true }}
  save: ${{ github.event.inputs.save || true }}
  os: ${{ github.event.inputs.os || 'MiniMac_arm64' }}
  commit_ref: ${{ github.event.inputs.commit_ref || '' }}
  quandelibc_ref: ${{ github.event.inputs.quandelibc_ref || '' }}

jobs:
  build-quandelibc:
    if: env.quandelibc_ref != ''
    name: Build quandelibc "${{ env.quandelibc_ref }}" wheels for ${{ env.os }}
    runs-on: macos-latest

    steps:
      - if: runner.os == 'Linux'
        name: Initialize python_v_cp linux
        run: |
          echo "python_v_cp=cp$( echo '${{env.python_v}}' | sed 's/\.\([0-9]\)/\1/' )" >> $GITHUB_ENV
      - if: runner.os != 'Linux'
        name: Initialize python_v_cp notLinux
        run: |
          echo "python_v_cp=cp$( echo '${{env.python_v}}' | sed 's/\.\([0-9]\)/\1/' )" >> $GITHUB_ENV
        shell: Bash

      - uses: actions/checkout@v3
        with:
          repository: Quandela/QuandeLibC
          ref: ${{ env.quandelibc_ref }}
          path: quandelibc
          submodules: recursive
          fetch-depth: 0

      - name: Set up Python ${{ env.python_v }}
        uses: actions/setup-python@v3
        with:
          python-version: ${{ env.python_v }}

      - name: Build wheels for ${{ env.PYTHON_V_CP }}
        env:
          CIBW_BUILD: '${{ env.PYTHON_V_CP }}-*'
          CIBW_SKIP: '*-musllinux_*'
          CIBW_ARCHS_LINUX: x86_64
          CIBW_ARCHS_MACOS: arm64
          CIBW_ARCHS_WINDOWS: AMD64
          CIBW_BUILD_VERBOSITY: 1
        run: |
          cd quandelibc
          python -m pip install cibuildwheel
          python -m cibuildwheel --output-dir precompiled-quandelibc

      - name: Store wheel
        uses: actions/upload-artifact@v2
        with:
          name: python-package-distributions
          path: quandelibc/precompiled-quandelibc

  benchmark:
    name: Run pytest-benchmark benchmark example
    if: ${{ always() }}
    needs:
      - build-quandelibc
    runs-on: ${{ env.os }}
    steps:
      - name: checkout on perceval's ref
        uses: actions/checkout@v2
        with:
          ref: ${{ env.commit_ref }}
          fetch-depth: 0

      # install python, already DL on MiniMac_arm64
      - if:  ${{ env.os != 'MiniMac_arm64' }}
        name: setup python
        uses: actions/setup-python@v2
        with:
          python-version: ${{ env.python_v }}

      - name: setup virtual env
        uses: syphar/restore-virtualenv@v1
        id: cache-virtualenv

      - if: env.quandelibc_ref != ''
        name: Fetch previously compiled Quandelibc
        uses: actions/download-artifact@v2
        with:
          name: python-package-distributions
          path: precompiled-quandelibc

      - if: env.quandelibc_ref != ''
        name: Install previously compiled Quandelibc
        run: |
          python -m pip uninstall -y quandelibc
          bash -c "python -m pip install precompiled-quandelibc/*macosx_11_0_arm64.whl"

      - name: Install dependencies
        run: |
            python -m pip install --upgrade pip
            python -m pip install -r requirements.txt pytest pytest-benchmark
            python -m pip install .

      # save var env use bash shell for notLinux
      - if: runner.os == 'Linux'
        name: setup var_env on linux
        run: |
          echo "folder_env=${{ env.os }}-CPython-${{ env.python_v }}" >> $GITHUB_ENV
          echo "folder_file_json=.benchmarks/${{ env.os }}-CPython-${{ env.python_v }}/log/${{ github.run_number }}_$( git describe --tags )_${{ github.sha }}.json" >> $GITHUB_ENV
      - if: runner.os != 'Linux'
        name: setup var_env on notLinux
        run: |
          echo "folder_env=${{ env.os }}-CPython-${{ env.python_v }}" >> $GITHUB_ENV
          echo "folder_file_json=.benchmarks/${{ env.os }}-CPython-${{ env.python_v }}/log/${{ github.run_number }}_$( git describe --tags )_${{ github.sha }}.json" >> $GITHUB_ENV
        shell: Bash

      - name: Run benchmark
        run: |
          python -m pytest benchmark/benchmark_*.py --benchmark-json out.json --benchmark-storage file://./.benchmarks/${{ env.folder_env }}/log/ 
          mv out.json ${{ env.folder_file_json }}

      # use github-action-benchmark for graph
      - if: github.event_name == 'push'
        name: compare result with last version and save on current branch
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Automated report
          tool: 'pytest'
          output-file-path: ${{ env.folder_file_json }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          benchmark-data-dir-path: .benchmarks/${{ env.folder_env }}
          gh-pages-branch: 'main'
          auto-push: ${{ env.save }}
          alert-threshold: '120%'
          comment-on-alert: true

      - if: github.event_name == 'workflow_dispatch'
        name: compare result with last version and save on current branch
        uses: benchmark-action/github-action-benchmark@v1
        with:
          name: Automated report
          tool: 'pytest'
          output-file-path: ${{ env.folder_file_json }}
          github-token: ${{ secrets.GITHUB_TOKEN }}
          benchmark-data-dir-path: .benchmarks/${{ env.folder_env }}
          gh-pages-branch: 'main'
          gh-repository:
          auto-push: ${{ env.save }}
          alert-threshold: '120%'
          comment-on-alert: true

      - name: upload the log result
        uses: actions/upload-artifact@v3
        with:
          name: pytest_benchmarks_graph_${{ env.folder_env }}_${{ github.run_number }}
          path: |
            .benchmarks/${{ env.folder_env }}

      - name: Download the log result
        uses: actions/download-artifact@v3
        with:
          name: pytest_benchmarks_graph_${{ env.folder_env }}_${{ github.run_number }}
          path: .benchmarks/${{ env.folder_env }}